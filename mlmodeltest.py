# -*- coding: utf-8 -*-
"""MLModelTest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12DkmnKYM32tvfcST4229hKywPcf2SiXq
"""

import pandas as pd
import numpy as np
def fetch_data(ticker, start_date, end_date):
    """
    Fetch historical data for the given ticker.

    Parameters:
        ticker (str): Stock ticker symbol.
        start_date (str): Start date (YYYY-MM-DD).
        end_date (str): End date (YYYY-MM-DD).

    Returns:
        pd.DataFrame: Historical OHLCV data with returns.
    """
    import yfinance as yf
    data = yf.download(ticker, start=start_date, end=end_date)
    data['Returns'] = data['Adj Close'].pct_change()
    return data.dropna()

'''
Buy Signal: "If the price is expected to rise significantly in the next lookahead days, I’ll buy now."
Sell Signal: "If the price is expected to drop significantly in the next lookahead days, I’ll sell now."
Hold Signal: "If the price movement is small, I’ll hold and avoid trading."
'''
def generate_swing_labels(data, lookahead=5, threshold=0.05):
    """
    Generate swing trading labels based on future returns.

    Parameters:
        data (pd.DataFrame): Historical price data with 'Close' column.
        lookahead (int): Number of future days to calculate returns.
        threshold (float): Percent return threshold for labeling.

    Returns:
        pd.DataFrame: Data with 'Signal' column for swing trading.
    """
    # Calculate future returns
    data['Future_Returns'] = data['Close'].shift(-lookahead) / data['Close'] - 1

    # Initialize signals
    data['Signal'] = 0

    # Label buy and sell signals
    data.loc[data['Future_Returns'] > 0.05, 'Signal'] = 1  # Buy if > 5% future return
    data.loc[data['Future_Returns'] < -0.05, 'Signal'] = -1  # Sell if < -5% future return
    data.loc[(data['Future_Returns'] <= 0.05) & (data['Future_Returns'] >= -0.05), 'Signal'] = 0  # Hold otherwise

    return data.dropna()  # Drop rows with NaN Future_Returns

!pip3 install ta
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from ta.momentum import RSIIndicator  # Example feature

def add_rsi_feature(data, window=14):
    """
    Add RSI (Relative Strength Index) as a feature.

    Parameters:
        data (pd.DataFrame): Historical price data with 'Close' column.
        window (int): Lookback period for RSI.

    Returns:
        pd.DataFrame: Data with RSI feature.
    """
    data['RSI'] = RSIIndicator(data['Close'].squeeze(), window=window).rsi()
    return data.dropna()

if __name__ == "__main__":
    # Fetch Data
    ticker = "AAPL"
    start_date = "2000-01-01"
    end_date = "2023-01-01"
    data = fetch_data(ticker, start_date, end_date)

    data = generate_swing_labels(data, lookahead=5, threshold=0.03)
    # Step 3: Balance the Data
    from sklearn.utils import resample

    # Separate the data by class
    data_majority = data[data['Signal'] == 0]   # Hold
    data_minority = data[data['Signal'] != 0]  # Buy/Sell

    # Upsample the minority classes to match the majority class size
    data_minority_upsampled = resample(
        data_minority,
        replace=True,
        n_samples=len(data_majority),  # Match the majority class size
        random_state=42
    )

    # Combine the balanced dataset
    data_balanced = pd.concat([data_majority, data_minority_upsampled])

    # Shuffle the data to mix classes
    data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

    print(data['Signal'].value_counts(normalize=True))  # Proportion of each signal


    # Step 2: Add RSI Feature
    data = add_rsi_feature(data)
    for lag in range(1, 4):  # Try lags for the past 3 days
        data[f'Lag_{lag}'] = data['Returns'].shift(lag)
    data['Volatility'] = data['Returns'].rolling(window=20).std()  # Add Volatility
    data['SMA_50'] = data['Close'].rolling(window=50).mean()
    data['SMA_200'] = data['Close'].rolling(window=200).mean()
    data['SMA_Crossover'] = np.where(data['SMA_50'] > data['SMA_200'], 1, -1)
    from ta.trend import ADXIndicator
    adx = ADXIndicator(data['High'].squeeze(), data['Low'].squeeze(), data['Close'].squeeze(), window=14)
    data['ADX'] = adx.adx()
    from ta.trend import PSARIndicator
    psar = PSARIndicator(data['High'].squeeze(), data['Low'].squeeze(), data['Close'].squeeze(), step=0.02, max_step=0.2)
    data['PSAR'] = psar.psar()
    # Step 3: Prepare Features and Target
    features = ['RSI', 'Lag_1', 'Lag_2', 'Lag_3', 'Volatility', 'SMA_Crossover', 'ADX', 'PSAR']
    X = data[features]
    y = data['Signal']

    # Step 4: Split Data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
    X_train.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in X_train.columns]
    X_test.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in X_test.columns]

    X_train.columns = X_train.columns.str.strip().str.replace(r'[^\w]', '_', regex=True)
    X_test.columns = X_test.columns.str.strip().str.replace(r'[^\w]', '_', regex=True)
    # Step 5: Train Model
    model = LGBMClassifier(n_estimators=100, max_depth=5, class_weight={0: 1.0, 1: 5.0, -1: 5.0})
    model.fit(X_train, y_train)

    # Step 6: Evaluate Model
    y_pred = model.predict(X_test)
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # Step 7: Document Results
    print("Feature Importance:")
    for feature, importance in zip(features, model.feature_importances_):
        print(f"{feature}: {importance}")

import requests
import pandas as pd

def fetch_news_sentiment(ticker, api_key):
    """
    Fetch news sentiment data for a specific ticker using Alpha Vantage's NEWS_SENTIMENT API.

    Parameters:
        ticker (str): Stock ticker symbol.
        api_key (str): Alpha Vantage API key.

    Returns:
        pd.DataFrame: DataFrame containing sentiment scores and timestamps.
    """
    url = f"https://www.alphavantage.co/query"
    params = {
        "function": "NEWS_SENTIMENT",
        "tickers": ticker,
        "apikey": api_key
    }
    response = requests.get(url, params=params)
    data = response.json()

    # Check for errors in the response
    if "feed" not in data:
        print("Error fetching data:", data.get("Note", "Unknown error"))
        return pd.DataFrame()

    # Parse sentiment data
    sentiment_data = []
    for item in data['feed']:
        sentiment_data.append({
            "timestamp": item['time_published'],
            "ticker": ticker,
            "overall_sentiment_score": item.get('overall_sentiment_score', None),
            "positive_score": item.get('positive_score', None),
            "negative_score": item.get('negative_score', None)
        })

    return pd.DataFrame(sentiment_data)

# Example Usage
api_key = "1D2RRCTS25P9OEKG"
ticker = "AAPL"
news_sentiment_df = fetch_news_sentiment(ticker, api_key)
print(news_sentiment_df.shape)

import pandas as pd
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import yfinance as yf
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator, EMAIndicator
from ta.volume import VolumeWeightedAveragePrice

# Fetch Data
def fetch_data(ticker, start_date, end_date):
    data = yf.download(ticker, start=start_date, end=end_date)
    data['Returns'] = data['Adj Close'].pct_change()
    return data.dropna()

# Generate Binary Labels for Buy/Sell
def generate_trade_labels(data, lookahead=5, buy_threshold=0.03, sell_threshold=-0.03):
    data['Future_Returns'] = data['Close'].shift(-lookahead) / data['Close'] - 1
    data['Trade_Signal'] = 0
    data.loc[data['Future_Returns'] > buy_threshold, 'Trade_Signal'] = 1
    data.loc[data['Future_Returns'] < sell_threshold, 'Trade_Signal'] = -1
    return data.dropna()

# Market Labelling
def add_market_labels(data, market_data, vix_data):
    market_data['Market_Returns'] = market_data['Adj Close'].pct_change()
    market_data['Market_Volatility'] = market_data['Market_Returns'].rolling(window=20).std()
    vix_data['VIX_Change'] = vix_data['Adj Close'].pct_change()

    # Merge with stock data
    data = data.merge(market_data[['Market_Returns', 'Market_Volatility']], left_index=True, right_index=True, how='left')
    data = data.merge(vix_data[['VIX_Change']], left_index=True, right_index=True, how='left')

    # Market condition labeling
    data['Market_Condition'] = np.where(data['Market_Returns'] > 0.02, 'Bullish',
                                        np.where(data['Market_Returns'] < -0.02, 'Bearish', 'Neutral'))
    data['Market_Condition'] = data['Market_Condition'].astype('category')
    return data

# Add Features
def add_features(data):
    # RSI
    data['RSI'] = RSIIndicator(data['Close'].squeeze()).rsi()

    # Lagged Returns
    for lag in range(1, 4):
        data[f'Lag_{lag}'] = data['Returns'].shift(lag)

    # Volatility
    data['Volatility'] = data['Returns'].rolling(window=20).std()

    # SMA Crossover
    data['SMA_50'] = SMAIndicator(data['Close'].squeeze(), window=50).sma_indicator()
    data['SMA_200'] = SMAIndicator(data['Close'].squeeze(), window=200).sma_indicator()
    data['SMA_Crossover'] = np.where(data['SMA_50'] > data['SMA_200'], 1, -1)

    # Momentum: Rate-of-Change (ROC)
    data['Price_ROC'] = data['Close'].pct_change(periods=10)
    data['Volume_ROC'] = data['Volume'].pct_change(periods=10)

    # Relative Strength of Volume
    data['RVOL'] = data['Volume'] / data['Volume'].rolling(window=50).mean()

    # Volume-Weighted Moving Average
    vwap = VolumeWeightedAveragePrice(
        data['High'].squeeze(), data['Low'].squeeze(), data['Close'].squeeze(), data['Volume'].squeeze()
    )
    data['VWMA'] = vwap.volume_weighted_average_price()

    return data.dropna()

# Add Sector Performance Data
def add_sector_features(data, sector_ticker):
    sector_data = fetch_data(sector_ticker, start_date="2000-01-01", end_date="2023-01-01")
    sector_data['Sector_Returns'] = sector_data['Adj Close'].pct_change()
    data = data.merge(sector_data[['Sector_Returns']], left_index=True, right_index=True, how='left')
    return data
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV
# Fetch Earnings Data within Date Range
def fetch_earnings_data(ticker, start_date, end_date):
    url = f"https://financialmodelingprep.com/api/v3/historical/earning_calendar/{ticker}?apikey=9fRw7yTqUg7R7nCT9e7qY5SDAK9Z5zok"
    response = requests.get(url)

    # Check if the API response is valid
    if response.status_code != 200 or not response.json():
        print(f"No earnings data available for {ticker}.")
        return pd.DataFrame()

    earnings = pd.DataFrame(response.json())

    # Check if the DataFrame is empty or missing required columns
    if earnings.empty or 'date' not in earnings:
        print(f"No earnings data available for {ticker}.")
        return pd.DataFrame()

    # Convert date to datetime and filter by the provided date range
    earnings['date'] = pd.to_datetime(earnings['date'])
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    earnings = earnings[(earnings['date'] >= start_date) & (earnings['date'] <= end_date)]

    # Select and clean relevant columns
    earnings = earnings[['date', 'eps', 'epsEstimated', 'revenue', 'revenueEstimated', 'time']]
    earnings.set_index('date', inplace=True)
    return earnings

# Generate Binary Labels for Buy/Sell
def generate_trade_labels(data, lookahead=5, buy_threshold=0.03, sell_threshold=-0.03):
    data['Future_Returns'] = data['Close'].shift(-lookahead) / data['Close'] - 1
    data['Trade_Signal'] = 0
    data.loc[data['Future_Returns'] > buy_threshold, 'Trade_Signal'] = 1
    data.loc[data['Future_Returns'] < sell_threshold, 'Trade_Signal'] = -1
    return data.dropna()

# Add Earnings Features
def add_earnings_features(data, earnings):
    # Standardize indices to UTC
    data.index = pd.to_datetime(data.index).tz_convert('UTC')
    earnings.index = pd.to_datetime(earnings.index).tz_localize('UTC')

    # Ensure no duplicate indices
    data = data[~data.index.duplicated()]
    earnings = earnings[~earnings.index.duplicated()]

    # Rename stock data index to match earnings index
    data.index.name = 'date'
    earnings.index.name = 'date'

    # Reset index for merging
    data = data.reset_index()
    earnings = earnings.reset_index()

    # Merge earnings data with stock data
    data = data.merge(earnings, on='date', how='left')

    # Set the index back to 'date'
    data.set_index('date', inplace=True)

    # Add earnings-related features
    data['Earnings_Event'] = (~data['eps'].isna()).astype(int)
    data['EPS_Surprise'] = (data['eps'] - data['epsEstimated']) / abs(data['epsEstimated'])
    data['Revenue_Surprise'] = (data['revenue'] - data['revenueEstimated']) / abs(data['revenueEstimated'])
    data['Earnings_Time'] = data['time'].fillna('none').astype('category')

    # Fill NaN for non-earnings days
    data.fillna({'EPS_Surprise': 0, 'Revenue_Surprise': 0}, inplace=True)

    return data
# Main Execution
if __name__ == "__main__":
    # Step 1: Fetch Data
    ticker = "AAPL"
    market_ticker = "^GSPC"
    vix_ticker = "^VIX"
    sector_ticker = "XLK"

    start_date = "2000-01-01"
    end_date = "2023-01-01"

    data = fetch_data(ticker, start_date, end_date)
    earnings_data = fetch_earnings_data(ticker, start_date, end_date)

    market_data = fetch_data(market_ticker, start_date, end_date)
    vix_data = fetch_data(vix_ticker, start_date, end_date)

    # Step 2: Generate Binary Labels
    data = generate_trade_labels(data, lookahead=5, buy_threshold=0.03, sell_threshold=-0.03)

    # Step 3: Add Features
    data = add_features(data)
    print("Stock Data Index:")
    print(data.index)

    # Check the index of the earnings data
    print("Earnings Data Index:")
    print(earnings_data.index)

    #data = add_earnings_features(data, earnings_data)
    # Step 4: Add Market Features
    data = add_market_labels(data, market_data, vix_data)

    # Step 5: Add Sector Features
    data = add_sector_features(data, sector_ticker)

    # Step 6: Filter to Buy/Sell Only
    data = data[data['Trade_Signal'] != 0]

    # Step 7: Prepare Features and Target
    features = ['RSI', 'Lag_1', 'Lag_2', 'Lag_3', 'Volatility', 'SMA_Crossover',
                'Price_ROC', 'Volume_ROC', 'RVOL', 'VWMA',
                'Market_Returns', 'Market_Volatility', 'VIX_Change', 'Sector_Returns']
                #'EPS_Surprise', 'Revenue_Surprise', 'Earnings_Event']
        # Add Nonlinear Interaction Features
    data['RSI_Volatility'] = data['RSI'] * data['Volatility']
    data['Lag1_MarketReturns'] = data['Lag_1'] * data['Market_Returns']
    data['PriceROC_VolumeROC'] = data['Price_ROC'] * data['Volume_ROC']

    # Update features to include interaction terms
    features += ['RSI_Volatility', 'Lag1_MarketReturns', 'PriceROC_VolumeROC']

    X = data[features]
    y = data['Trade_Signal']

    # Step 8: Handle Class Imbalance with SMOTE
    smote = SMOTE(random_state=42, sampling_strategy='auto')
    X_resampled, y_resampled = smote.fit_resample(X, y)

    # Step 9: Split Data
    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

    # Handle Column Formatting
    X_train.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in X_train.columns]
    X_test.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in X_test.columns]
    X_train.columns = X_train.columns.str.strip().str.replace(r'[^\w]', '_', regex=True)
    X_test.columns = X_test.columns.str.strip().str.replace(r'[^\w]', '_', regex=True)

  # Hyperparameter Tuning for Random Forest
    rf_param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    rf_grid_search = GridSearchCV(
        estimator=RandomForestClassifier(random_state=42),
        param_grid=rf_param_grid,
        scoring='accuracy',
        cv=3,
        verbose=1,
        n_jobs=-1
    )
    rf_grid_search.fit(X_train, y_train)
    rf_best_model = rf_grid_search.best_estimator_
    print("Random Forest Best Parameters:", rf_grid_search.best_params_)
    print("Random Forest Classification Report:")
    print(classification_report(y_test, rf_best_model.predict(X_test)))

    # Hyperparameter Tuning for Gradient Boosting
    gb_param_grid = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    gb_grid_search = GridSearchCV(
        estimator=GradientBoostingClassifier(random_state=42),
        param_grid=gb_param_grid,
        scoring='accuracy',
        cv=3,
        verbose=1,
        n_jobs=-1
    )
    gb_grid_search.fit(X_train, y_train)
    gb_best_model = gb_grid_search.best_estimator_
    print("Gradient Boosting Best Parameters:", gb_grid_search.best_params_)
    print("Gradient Boosting Classification Report:")
    print(classification_report(y_test, gb_best_model.predict(X_test)))

    # Hyperparameter Tuning for LightGBM
    lgbm_param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1, 0.2],
        'num_leaves': [20, 31, 40],
        'min_child_samples': [10, 20, 30]
    }
    lgbm_grid_search = GridSearchCV(
        estimator=LGBMClassifier(random_state=42),
        param_grid=lgbm_param_grid,
        scoring='accuracy',
        cv=3,
        verbose=1,
        n_jobs=-1
    )
    lgbm_grid_search.fit(X_train, y_train)
    lgbm_best_model = lgbm_grid_search.best_estimator_
    print("LightGBM Best Parameters:", lgbm_grid_search.best_params_)
    print("LightGBM Classification Report:")
    print(classification_report(y_test, lgbm_best_model.predict(X_test)))

    # Ensemble VotingClassifier with best models
    ensemble_model = VotingClassifier(
        estimators=[('rf', rf_best_model), ('gb', gb_best_model), ('lgbm', lgbm_best_model)],
        voting='soft'
    )
    ensemble_model.fit(X_train, y_train)

    # Evaluate Ensemble Model
    print("Ensemble Classification Report:")
    print(classification_report(y_test, ensemble_model.predict(X_test)))
    print("Ensemble Confusion Matrix:")
    print(confusion_matrix(y_test, ensemble_model.predict(X_test)))

Ensemble Classification Report:
              precision    recall  f1-score   supportffet

          -1       0.79      0.78      0.78       317
           1       0.77      0.79      0.78       309

    accuracy                           0.78       626
   macro avg       0.78      0.78      0.78       626
weighted avg       0.78      0.78      0.78       626

Ensemble Confusion Matrix:
[[246  71]
 [ 66 243]]